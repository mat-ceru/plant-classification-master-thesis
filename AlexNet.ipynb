{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MbtUcKfE_I4g",
        "fxs_3zcG_NZd",
        "ONPuly4naKdu",
        "rgeaxKMtcuBK",
        "gRuwnREyNWEe",
        "uvABcepY_Vfe",
        "_N9hKn1aB5Ow",
        "GXEqPr_856U6",
        "0Q4ce5dMTqFc",
        "aC2PZhMSUIAH",
        "VSBp-VgJUlZ7",
        "fA5PVBitPLOf",
        "kN18qjPHlbfQ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbtUcKfE_I4g"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzg4cO9xLvUG"
      },
      "source": [
        "!pip3 install 'torch'\n",
        "!pip3 install 'torchvision'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n",
        "!pip3 install 'dropbox'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxs_3zcG_NZd"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7N0hU-VLx8W"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import dropbox\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONPuly4naKdu"
      },
      "source": [
        "## General Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXuQ17tzaIRF"
      },
      "source": [
        "# Path where the zipped dropbox repo is downloaded\n",
        "DRONUTS_ZIP_PATH = \"./dronuts.zip\"\n",
        "\n",
        "# Path where the unzipped dropbox repository is saved\n",
        "DRONUTS_PATH = \"./DRONUTS\"\n",
        "\n",
        "# Path of root for download on dropbox side\n",
        "DRONUTS_DROPBOX_PATH = \"/dronuts\"\n",
        "\n",
        "# Path of generated dataset\n",
        "DATASET_PATH = \"./dataset\"\n",
        "\n",
        "# Prefix for distinguish useful dataset images\n",
        "IMAGE_PREFIX = \"DJI_\"\n",
        "\n",
        "# Particular path where remove all images, excluding metadata file\n",
        "IMAGES_TO_REMOVE_PATH = \"./DRONUTS/Dogliani_C001/2021-05-27\"\n",
        "\n",
        "# Data for configuring machine learning algorithms\n",
        "# Number of classes of dataset\n",
        "NUM_CLASSES = 2\n",
        "# Device on which the algorithms are trained and tested\n",
        "DEVICE = 'cuda'\n",
        "# Fixed hyperparameters for NN stochastic gradient\n",
        "MOMENTUM = 0.9\n",
        "GAMMA = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgeaxKMtcuBK"
      },
      "source": [
        "## Define drive for save output files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QvP1g_WnpPK"
      },
      "source": [
        "In order to allow the mounting, in the output console is necessary to follow the istructions that permits to insert an authentication token.\n",
        "\n",
        "The mounting give not permission for create directory. For that reason the OUTPUT_FILE_DIRECTORY has to be created before on own drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcD3aiBseTth"
      },
      "source": [
        "OUTPUT_FILE_DIRECTORY = \"./gdrive/MyDrive/ColabOutput/Tesi\"\n",
        "\n",
        "if not os.path.isdir(OUTPUT_FILE_DIRECTORY):\n",
        "  drive.mount('/content/gdrive')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khzotb4eaS4U"
      },
      "source": [
        "## Import dropbox folder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following part the dropbox repository is downloaded and unpacked, in order to extract the images useful for the dataset.\n",
        "After unpack, the zipped file is removed."
      ],
      "metadata": {
        "id": "IUq1vJsTeZF7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0CPN8oAaT9n"
      },
      "source": [
        "def unpack():\n",
        "  with zipfile.ZipFile(DRONUTS_ZIP_PATH, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "if not os.path.isfile(DRONUTS_ZIP_PATH) and not os.path.isdir(DRONUTS_PATH):\n",
        "  dbx = dropbox.Dropbox() # Insert as parameter the dropbox app token\n",
        "  dbx.files_download_zip_to_file(DRONUTS_ZIP_PATH, DRONUTS_DROPBOX_PATH)\n",
        "\n",
        "if os.path.isfile(DRONUTS_ZIP_PATH) and not os.path.isdir(DRONUTS_PATH):\n",
        "  unpack()\n",
        "  os.remove(DRONUTS_ZIP_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In IMAGES_TO_REMOVE_PATH are removed all the files, excluding *dettagli_immagini.txt* useful for metadata. Custom logic for remove uncut images, not suitable for dataset."
      ],
      "metadata": {
        "id": "UJCbIKksfTK8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9396Hs5lR9Wq"
      },
      "source": [
        "if os.path.isdir(IMAGES_TO_REMOVE_PATH):\n",
        "  for file in os.listdir(IMAGES_TO_REMOVE_PATH):\n",
        "    file_path = f'{IMAGES_TO_REMOVE_PATH}/{file}'\n",
        "    if os.path.isfile(file_path) and file != \"dettagli_immagini.txt\":\n",
        "      os.remove(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRuwnREyNWEe"
      },
      "source": [
        "## Define dataset directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIJojyl4cDtQ"
      },
      "source": [
        "# In case of error in the next section, permits to remove all directory in order to rerun it\n",
        "# shutil.rmtree(DATASET_PATH)\n",
        "# shutil.rmtree(DRONUTS_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIN6dKoPNkXd"
      },
      "source": [
        "def find_file_descriptor(actual_path: str):\n",
        "  for file_path in dettagli_immagini_by_path.keys():\n",
        "    if actual_path.__contains__(file_path):\n",
        "      return dettagli_immagini_by_path[file_path]\n",
        "\n",
        "def define_class_from_labels(label_fisio: str, label_pato: str):\n",
        "  return str(int(label_fisio) | int(label_pato))\n",
        "\n",
        "def get_label(original_image_name: str, dir_name: str):\n",
        "  lines = find_file_descriptor(dir_name)[\"lines\"]\n",
        "  label_fisio_index = lines[0].lower().split('\\t').index(\"label_fisio_(sano-0/malato-1)\")\n",
        "  label_pato_index = lines[0].lower().split('\\t').index(\"label_pato_(sano-0/malato-1)\")\n",
        "  name_index = lines[0].lower().split('\\t').index(\"nome_immagine\")\n",
        "  for line in lines:\n",
        "    splitted_line = line.split('\\t')\n",
        "    if splitted_line[name_index] == original_image_name:\n",
        "       return define_class_from_labels(splitted_line[label_fisio_index], splitted_line[label_pato_index])\n",
        "\n",
        "def set_label(label: str, image_name: str):\n",
        "  if image_name not in labels_by_image_name:\n",
        "    labels_by_image_name[image_name] = []\n",
        "  labels_by_image_name[image_name].append(label)\n",
        "\n",
        "def get_extension(file_name: str):\n",
        "  return file_name[file_name.find('.') + 1:]\n",
        "\n",
        "def get_label_info(file: str, dir_name: str):\n",
        "  fd = open(f'{dir_name}/{file}', 'r')\n",
        "  dettagli_immagini_by_path[dir_name] = {\n",
        "      \"lines\": fd.readlines(),\n",
        "      \"file_path\": f'{dir_name}/{file}'\n",
        "  }\n",
        "  fd.close()\n",
        "\n",
        "def manage_copy_for_dataset(file: str, dirpath: str, label: str):\n",
        "  base_dest_path = f'{DATASET_PATH}/{label}'\n",
        "  if not os.path.isdir(base_dest_path):\n",
        "    os.mkdir(base_dest_path)\n",
        "  \n",
        "  source_path = f'{dirpath}/{file}'\n",
        "  dest_file = file\n",
        "  dest_path = f'{base_dest_path}/{file}'\n",
        "  while os.path.isfile(dest_path):\n",
        "    dest_path = dest_path.replace(\".jpg\", \"_c.jpg\")\n",
        "    dest_path = dest_path.replace(\".JPG\", \"_c.JPG\")\n",
        "    dest_file = dest_file.replace(\".jpg\", \"_c.jpg\")\n",
        "    dest_file = dest_file.replace(\".JPG\", \"_c.JPG\")\n",
        "  shutil.copy(source_path, dest_path)\n",
        "  return dest_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the dataset from the unzipped dropbox repository. \n",
        "At beginning remove possible wrong dataset directory.\n",
        "After navigates the unzipped dropbox repository in order to extract for each image the right label, taken from metadata file *dettagli_immagini.txt*, in order to create for each possible label a directory in DATASET_PATH.\n",
        "The images put in dataset are filtered by extension (jpg) and prefix (IMAGE_PREFIX)."
      ],
      "metadata": {
        "id": "ei4pbQkef3wV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7VRu03Bj9L2"
      },
      "source": [
        "dettagli_immagini_by_path = {}; labels_by_image_name = {}\n",
        "\n",
        "if os.path.isdir(DATASET_PATH):\n",
        "  shutil.rmtree(DATASET_PATH)\n",
        "\n",
        "os.mkdir(DATASET_PATH)\n",
        "for dir_path, _, files in os.walk(DRONUTS_PATH):\n",
        "  if \"dettagli_immagini.txt\" in files:\n",
        "    get_label_info(\"dettagli_immagini.txt\", dir_path)\n",
        "  for file in files:\n",
        "    if get_extension(file).lower() == \"jpg\" and file.startswith(IMAGE_PREFIX):\n",
        "      label = get_label(file, dir_path)\n",
        "      new_file_name = manage_copy_for_dataset(file, dir_path, label)\n",
        "      set_label(label, new_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDM_mib5igyJ"
      },
      "source": [
        "print(labels_by_image_name)\n",
        "\n",
        "# to check number of files\n",
        "for dir_path, _, files in os.walk(DATASET_PATH):\n",
        "  if len(files) > 0:\n",
        "    print(f'{dir_path}: {str(len(files))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvABcepY_Vfe"
      },
      "source": [
        "## Base functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vztVCv3fQXjR"
      },
      "source": [
        "def get_base_datasets():\n",
        "  # Transformation in order to use the NN with pretrained weights\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  compose = transforms.Compose([transforms.Resize((1300, 1600)), transforms.ToTensor(), normalize])\n",
        "\n",
        "  all_dataset = torchvision.datasets.ImageFolder(DATASET_PATH, transform=compose)\n",
        "  return get_subsets(all_dataset, 5)\n",
        "\n",
        "def get_subsets(base_set: torchvision.datasets.ImageFolder, splitting_ratio: int):\n",
        "  s1_indexes = [idx for idx in range(len(base_set)) if idx % splitting_ratio]\n",
        "  s2_indexes = [idx for idx in range(len(base_set)) if not idx % splitting_ratio]\n",
        "  s1_set = Subset(base_set, s1_indexes)\n",
        "  s2_set = Subset(base_set, s2_indexes)\n",
        "\n",
        "  return s1_set, s2_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBm16_-Jq8Mt"
      },
      "source": [
        "def calculate_matrix_metrics(actual_labels, predictions, true_positive, true_negative, false_positive, false_negative):\n",
        "  for index, prediction in enumerate(predictions):\n",
        "    if actual_labels[index] == 1 and prediction == 1:\n",
        "      true_positive += 1\n",
        "    elif actual_labels[index] == 1 and prediction == 0:\n",
        "      false_negative += 1\n",
        "    elif actual_labels[index] == 0 and prediction == 1:\n",
        "      false_positive += 1\n",
        "    elif actual_labels[index] == 0 and prediction == 0:\n",
        "      true_negative += 1\n",
        "\n",
        "  return true_positive, true_negative, false_positive, false_negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLnzTLI0_CTA"
      },
      "source": [
        "def test_log(ta, tl, ttp, ttn, tfp, tfn, output_file=sys.stdout):\n",
        "  print(f\"\"\"test_accuracy: {test_accuracy}, test_loss: {test_loss},\n",
        "            test_true_positive {test_true_positive}, \n",
        "            test_true_negative: {test_true_negative}, \n",
        "            test_false_positive: {test_false_positive},\n",
        "            test_false_negative: {test_false_negative}\"\"\", \n",
        "        file=output_file)\n",
        "\n",
        "def train_val_log(ta, tl, ttp, ttn, tfp, tfn, va, vl, vtp, vtn, vfp, vfn, epoch, tot_epochs, output_file=sys.stdout):\n",
        "  print(f\"\"\"train_accuracy: {ta}, train_loss: {tl}, train_true_positive: {ttp}, \n",
        "            train_true_negative: {ttn}, train_false_positive: {tfp}, \n",
        "            train_false_negative : {tfn}, val_acc: {va}, val_loss: {vl},\n",
        "            val_true_positive: {vtp}, val_true_negative: {vtn},\n",
        "            val_false_positive: {vfp}, val_false_negative: {vfn}\n",
        "            ({epoch} / {tot_epochs})\"\"\", file=output_file)\n",
        "  \n",
        "def train_log(ta, tl, ttp, ttn, tfp, tfn, epoch, tot_epochs, output_file=sys.stdout):\n",
        "    print(f\"\"\"train_acc: {ta}, train_loss: {tl}, train_true_positive: {ttp},\n",
        "              train_true_negative: {ttn}, train_false_positive: {tfp},\n",
        "              train_false_negative : {tfn}, ({epoch} / {tot_epochs})\"\"\", file=output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXzuaCLqSoBo"
      },
      "source": [
        "PLOT_COLORS = [\"orangered\", \"limegreen\", \"lightseagreen\", \"navy\", \"gold\", \"magenta\"]\n",
        "\n",
        "def plot_multiple_line_graphic(data_arrays, labels, output_file=None):\n",
        "    fig, ax = plt.subplots()\n",
        "    for index, data_array in enumerate(data_arrays):\n",
        "      ax.plot(data_array, label=labels[index], color=PLOT_COLORS[index % len(PLOT_COLORS)])\n",
        "    ax.legend()\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    \n",
        "    file_name = f\"{datetime.datetime.now()}.jpg\"\n",
        "    plt.savefig(f\"{OUTPUT_FILE_DIRECTORY}/{file_name}\")\n",
        "    print(file_name)\n",
        "    if output_file is not None:\n",
        "      print(file_name, file=output_file)\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NuKXbgvCJ5R"
      },
      "source": [
        "def train_network(net, parameters_to_optimize, learning_rate, num_epochs, \n",
        "                  batch_size, weight_decay, step_size, gamma, train_dataset, \n",
        "                  val_dataset=None, verbosity=False, plot=False, output_file=None):\n",
        "  \n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=learning_rate, momentum=MOMENTUM, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    net = net.to(DEVICE)\n",
        "    best_net = alexnet(pretrained=True)\n",
        "    best_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "\n",
        "    train_accuracies = []; train_losses = []; train_true_positives = []; train_true_negatives = [] \n",
        "    train_false_positives = []; train_false_negatives = []\n",
        "    val_accuracies = []; val_losses = []; val_true_positives = []; val_true_negatives = []\n",
        "    val_false_positives = []; val_false_negatives = []\n",
        "\n",
        "    best_val_accuracy = best_val_loss = 0.0\n",
        "    best_val_true_positive = best_val_true_negative = best_val_false_positive = 0\n",
        "    best_val_false_negative = 0\n",
        "\n",
        "    current_step = 0\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), \"Epochs\"):\n",
        "\n",
        "        train_running_corrects = train_true_positive = train_false_negative = 0\n",
        "        train_false_positive = train_true_negative = 0\n",
        "        sum_train_losses = 0.0\n",
        "\n",
        "        for images, labels in tqdm(train_dataloader, f\"Train dataLoader of epoch {epoch+1}\"):\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            net.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            train_true_positive, train_true_negative, train_false_positive, train_false_negative = \\\n",
        "              calculate_matrix_metrics(labels.data, \n",
        "                                        preds, \n",
        "                                        train_true_positive, \n",
        "                                        train_true_negative, \n",
        "                                        train_false_positive, \n",
        "                                        train_false_negative)\n",
        "            train_running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "            loss = criterion(outputs, labels)\n",
        "            sum_train_losses += loss.item()*images.size(0)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            current_step += 1\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if val_dataset is not None:\n",
        "            val_accuracy, val_loss, val_true_positive, val_true_negative, val_false_positive, val_false_negative = \\\n",
        "              test_network(net, val_dataset, TEST_BATCH_SIZE)\n",
        "            if val_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy\n",
        "                best_val_loss = val_loss\n",
        "                best_val_true_positive = val_true_positive \n",
        "                best_val_true_negative = val_true_negative\n",
        "                best_val_false_positive = val_false_positive\n",
        "                best_val_false_negative = val_false_negative\n",
        "                best_net.load_state_dict(net.state_dict())\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            val_losses.append(val_loss)\n",
        "            val_true_positives.append(val_true_positive)\n",
        "            val_true_negatives.append(val_true_negative)\n",
        "            val_false_positives.append(val_false_positive)\n",
        "            val_false_negatives.append(val_false_negative)\n",
        "\n",
        "        # Calculate accuracy on train set\n",
        "        train_accuracy = train_running_corrects / float(len(train_dataset))\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Calculate loss on training set\n",
        "        train_loss = sum_train_losses/float(len(train_dataset))\n",
        "        \n",
        "        train_losses.append(loss)\n",
        "        train_true_positives.append(train_true_positive) \n",
        "        train_true_negatives.append(train_true_negative) \n",
        "        train_false_positives.append(train_false_positive) \n",
        "        train_false_negatives.append(train_false_negative)\n",
        "\n",
        "        if verbosity:\n",
        "            if val_dataset is not None:\n",
        "              train_val_log(train_accuracy, train_loss, train_true_positive, \n",
        "                            train_true_negative, train_false_positive, \n",
        "                            train_false_negative, val_accuracy, val_loss, \n",
        "                            val_true_positive, val_true_negative, val_false_positive, \n",
        "                            val_false_negative, epoch + 1, num_epochs)\n",
        "              if output_file is not None:\n",
        "                train_val_log(train_accuracy, train_loss, train_true_positive, \n",
        "                              train_true_negative, train_false_positive, \n",
        "                              train_false_negative, val_accuracy, val_loss, \n",
        "                              val_true_positive, val_true_negative, val_false_positive, \n",
        "                              val_false_negative, epoch + 1, num_epochs, output_file)\n",
        "            else:\n",
        "              train_log(train_accuracy, train_loss, train_true_positive, \n",
        "                        train_true_negative, train_false_positive, \n",
        "                        train_false_negative, epoch + 1, num_epochs)\n",
        "              if output_file is not None:\n",
        "                train_log(train_accuracy, train_loss, train_true_positive, \n",
        "                          train_true_negative, train_false_positive, \n",
        "                          train_false_negative, epoch + 1, num_epochs, output_file)\n",
        "\n",
        "        scheduler.step()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if plot:\n",
        "      plot_multiple_line_graphic([train_losses, train_accuracies], \n",
        "                                 [\"Loss on training set\", \"Accuracy on training set\"],\n",
        "                                 output_file)\n",
        "\n",
        "      plot_data = [train_accuracies]\n",
        "      plot_labels = [\"Accuracy on training set\"]\n",
        "\n",
        "      if val_dataset is not None:\n",
        "        plot_multiple_line_graphic([val_losses, train_losses], \n",
        "                                   [\"Loss on validation set\", \"Loss on training set\"],\n",
        "                                   output_file)\n",
        "        plot_multiple_line_graphic([val_true_positives, val_true_negatives, val_false_positives, val_false_negatives],\n",
        "                                   [\"Validation true positives\", \"Validation true negatives\", \"Validation false positives\", \"Validation false negatives\"],\n",
        "                                   output_file)\n",
        "        plot_data.append(val_accuracies)\n",
        "        plot_labels.append(\"Accuracy on validation set\")\n",
        "\n",
        "      plot_multiple_line_graphic(plot_data, plot_labels, output_file)\n",
        "      plot_multiple_line_graphic([train_true_positives, train_true_negatives, train_false_positives, train_false_negatives],\n",
        "                                 [\"Training true positives\", \"Training true negatives\", \"Training false positives\", \"Traning false negatives\"],\n",
        "                                 output_file)\n",
        "\n",
        "    if val_dataset is None:\n",
        "      best_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    return best_net, best_val_accuracy, best_val_loss, best_val_true_positive, best_val_true_negative, best_val_false_positive, best_val_false_negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjzIyuSZIzJr"
      },
      "source": [
        "def test_network(net, test_dataset, batch_size):\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    net.train(False)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    sum_test_losses = 0.0\n",
        "    running_corrects = true_positive = false_negative = false_positive = true_negative = 0\n",
        "\n",
        "    for images, labels in tqdm(test_dataloader, f\"Test dataLoader\"):\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      test_loss = criterion(outputs, labels)\n",
        "      sum_test_losses += test_loss.item()*images.size(0)\n",
        "\n",
        "      # Update metrics\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "      true_positive, true_negative, false_positive, false_negative = \\\n",
        "        calculate_matrix_metrics(labels.data,\n",
        "                                  preds,\n",
        "                                  true_positive, \n",
        "                                  true_negative,\n",
        "                                  false_positive,\n",
        "                                  false_negative)\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "    # Calculate loss\n",
        "    test_loss = sum_test_losses / float(len(test_dataset))\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return accuracy, test_loss, true_positive, true_negative, false_positive, false_negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N9hKn1aB5Ow"
      },
      "source": [
        "## Random search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5ot-2QcFRmE"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SsVG8ZqFRRn"
      },
      "source": [
        "TENTATIVE_NUMBERS = 1\n",
        "BATCH_SIZE = 2\n",
        "TEST_BATCH_SIZE = 1\n",
        "EPOCHS_NUMBER = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ0nuyzUGGeG"
      },
      "source": [
        "### Dataset extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeOkUl3gGGEI"
      },
      "source": [
        "train_and_val_dataset, test_dataset = get_base_datasets()\n",
        "\n",
        "print('training & validation set {}'.format(len(train_and_val_dataset)))\n",
        "print('test set {}'.format(len(test_dataset)))\n",
        "\n",
        "train_dataset, val_dataset = get_subsets(train_and_val_dataset, 5)\n",
        "\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJtCRTfsGhRF"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkYpQf2IohDn"
      },
      "source": [
        "def single_iteration_log(iteration, lr, wd, gamma, va, vl, vtp, vtn, vfp, vfn, output_file=sys.stdout):\n",
        "  print(f\"Tentative {iteration}: lr {lr}, decay {wd}, gamma {gamma}\", file=output_file)\n",
        "  print(f\"\"\"val_accuracy: {va}, val_loss: {vl}, val_true_positive: {vtp},\n",
        "            val_true_negative: {vtn}, val_false_positive: {vfp}, \n",
        "            val_false_negative: {vfn} [{iteration} / {TENTATIVE_NUMBERS}]\"\"\", \n",
        "        file=output_file)\n",
        "  \n",
        "def final_log(bs, ba, bl, btp, btn, bfp, bfn, vas, vls, vtps, vtns, vfps, vfns, output_file=sys.stdout):\n",
        "  print(f\"\"\"{bs}, best_val_accuracy: {ba}, best_val_loss: {bl}, best_true_positives: {btp}, \n",
        "            best_true_negatives: {btn}, best_false_positives: {bfp}, best_false_negatives: {bfn}\"\"\", \n",
        "        file=output_file)\n",
        "  print(f\"val_accuracies: {val_accuracies}\", file=output_file)\n",
        "  print(f\"val_losses: {val_losses}\", file=output_file)\n",
        "  print(f\"val_true_positives: {val_true_positives}\", file=output_file)\n",
        "  print(f\"val_true_negatives: {val_true_negatives}\", file=output_file)\n",
        "  print(f\"val_false_positives: {val_false_positives}\", file=output_file)\n",
        "  print(f\"val_false_negatives: {val_false_negatives}\", file=output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSE19H6PB5h3"
      },
      "source": [
        "best_accuracy = best_loss = 0.0\n",
        "best_set = {}\n",
        "best_true_positives = best_true_negatives = best_false_positives = best_false_negatives = 0\n",
        "\n",
        "val_accuracies = []; val_losses = []; val_true_positives = []; val_true_negatives = []\n",
        "val_false_positives = []; val_false_negatives = []\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "with open(f\"{OUTPUT_FILE_DIRECTORY}/{datetime.datetime.now()}-output.txt\", \"w\") as f:\n",
        "  for i in tqdm(range(TENTATIVE_NUMBERS), \"Random search tentatives\"):\n",
        "    lr = random.uniform(0.0001, 0.0005)\n",
        "    weight_decay = random.uniform(0.0002, 0.0008)\n",
        "    gamma = 10**random.uniform(-2, 0)\n",
        "    actual_set = {\"lr\": lr, \"weight_decay\": weight_decay, \"gamma\": gamma}\n",
        "\n",
        "    net = alexnet(pretrained=True)\n",
        "    net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "    _, val_accuracy, val_loss, val_true_positive, val_true_negative, val_false_positive, val_false_negative = \\\n",
        "      train_network(net, net.parameters(), \n",
        "                    lr, EPOCHS_NUMBER, \n",
        "                    BATCH_SIZE, weight_decay, \n",
        "                    STEP_SIZE, gamma, \n",
        "                    train_dataset, \n",
        "                    val_dataset=val_dataset, \n",
        "                    verbosity=True,\n",
        "                    output_file=f,\n",
        "                    plot=True)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    val_true_positives.append(val_true_positive)\n",
        "    val_true_negatives.append(val_true_negative) \n",
        "    val_false_positives.append(val_false_positive) \n",
        "    val_false_negatives.append(val_false_negative)\n",
        "\n",
        "    if val_accuracy > best_accuracy:\n",
        "      best_accuracy = val_accuracy\n",
        "      best_loss = val_loss\n",
        "      best_true_positives = val_true_positive\n",
        "      best_true_negatives = val_true_negative\n",
        "      best_false_positives = val_false_positive\n",
        "      best_false_negatives = val_false_negative\n",
        "      best_set = copy.deepcopy(actual_set)\n",
        "    \n",
        "    single_iteration_log(i+1, lr, weight_decay, gamma, val_accuracy, val_loss, \n",
        "                         val_true_positive, val_true_negative, val_false_positive, \n",
        "                         val_false_negative, f)\n",
        "    \n",
        "    single_iteration_log(i+1, lr, weight_decay, gamma, val_accuracy, val_loss, \n",
        "                      val_true_positive, val_true_negative, val_false_positive, \n",
        "                      val_false_negative)\n",
        "    \n",
        "  final_log(best_set, best_accuracy, best_loss, best_true_positives, \n",
        "            best_true_negatives, best_false_positives, best_false_negatives,\n",
        "            val_accuracies, val_losses, val_true_positives, val_true_negatives, \n",
        "            val_false_positives, val_false_negatives, f)\n",
        "  \n",
        "  final_log(best_set, best_accuracy, best_loss, best_true_positives, \n",
        "          best_true_negatives, best_false_positives, best_false_negatives,\n",
        "          val_accuracies, val_losses, val_true_positives, val_true_negatives, \n",
        "          val_false_positives, val_false_negatives)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXEqPr_856U6"
      },
      "source": [
        "##Specific run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzSO9KM57Lnr"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeB1blOw6Oy_"
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "TEST_BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "LR = 0.00034017498684741146\n",
        "WEIGHT_DECAY = 0.00023276171907154138\n",
        "GAMMA = 0.09288936004816421"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLd8TXJI7R9E"
      },
      "source": [
        "### Dataset extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIalu7X27U9-"
      },
      "source": [
        "train_and_val_dataset, test_dataset = get_base_datasets()\n",
        "\n",
        "print('training & validation set {}'.format(len(train_and_val_dataset)))\n",
        "print('test set {}'.format(len(test_dataset)))\n",
        "\n",
        "train_dataset, val_dataset = get_subsets(train_and_val_dataset, 5)\n",
        "\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9gIXkwo7Y4G"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXY6MMuUw1RA"
      },
      "source": [
        "def final_log(bva, bvl, bvtp, bvtn, bvfp, bvfn, output_file=sys.stdout):\n",
        "  print(f\"\"\"best_val_accuracy: {bva}, best_val_loss: {bvl},\n",
        "            best_val_true_positive: {bvtp}, best_val_true_negative: {bvtn},\n",
        "            best_val_false_positive: {bvfp}, best_val_false_negative: {bvfn}\"\"\",\n",
        "        file=output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boy3ZCeD50IF"
      },
      "source": [
        "net = alexnet(pretrained=True)\n",
        "net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "with open(f\"{OUTPUT_FILE_DIRECTORY}/{datetime.datetime.now()}-output.txt\", \"w\") as f:\n",
        "  best_val_net, best_val_accuracy, best_val_loss, best_val_true_positive, best_val_true_negative, best_val_false_positive, best_val_false_negative = \\\n",
        "    train_network(net, net.parameters(), \n",
        "                  LR, NUM_EPOCHS, \n",
        "                  BATCH_SIZE, WEIGHT_DECAY, \n",
        "                  STEP_SIZE, GAMMA, \n",
        "                  train_dataset, \n",
        "                  val_dataset=val_dataset, \n",
        "                  verbosity=True, plot=True,\n",
        "                  output_file=f)\n",
        "\n",
        "  final_log(best_val_accuracy, best_val_loss, best_val_true_positive,\n",
        "            best_val_true_negative, best_val_false_positive, best_val_false_negative)\n",
        "  \n",
        "  final_log(best_val_accuracy, best_val_loss, best_val_true_positive,\n",
        "            best_val_true_negative, best_val_false_positive, \n",
        "            best_val_false_negative, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxj7-SlSKb_3"
      },
      "source": [
        "## Grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q4ce5dMTqFc"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyTQ7cRXTmaH"
      },
      "source": [
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 2\n",
        "TEST_BATCH_SIZE = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC2PZhMSUIAH"
      },
      "source": [
        "### Parameter ranges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fatgJ2hSUHiF"
      },
      "source": [
        "lr_range = [1e-5]\n",
        "weight_decay_range = [0, 1e-3, 1e-4, 1e-5]\n",
        "step_size_range = [15, 20, 35]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSBp-VgJUlZ7"
      },
      "source": [
        "### Dataset extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u407Oe7xUpSl"
      },
      "source": [
        "train_and_val_dataset, test_dataset = get_base_datasets()\n",
        "\n",
        "print('training & validation set {}'.format(len(train_and_val_dataset)))\n",
        "print('test set {}'.format(len(test_dataset)))\n",
        "\n",
        "train_dataset, val_dataset = get_subsets(train_and_val_dataset, 5)\n",
        "\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6fTm2sD_BOt"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXgFwGedXFUU"
      },
      "source": [
        "def single_iteration_log(set_, va, vl, vtp, vtn, vfp, vfn, output_file=sys.stdout):\n",
        "  print(f\"\"\"({set_}): val_accuracy: {va}, val_loss: {vl}, val_true_positive: {vtp},\n",
        "            val_true_negative: {vtn}, val_false_positive: {vfp}, \n",
        "            val_false_negative: {vfn}\"\"\", \n",
        "        file=output_file)\n",
        "  \n",
        "def final_log(best_set, ba, bl, btp, btn, bfp, bfn, all_sets, vas, vls, vtps, vtns, vfps, vfns, output_file=sys.stdout):\n",
        "  print(f\"\"\"({best_set}): best_val_accuracy: {ba}, best_val_loss: {bl}, best_true_positives: {btp}, \n",
        "            best_true_negatives: {btn}, best_false_positives: {bfp}, best_false_negatives: {bfn}\"\"\", \n",
        "        file=output_file)\n",
        "  print(f\"all_sets: {all_sets}\", file=output_file)\n",
        "  print(f\"val_accuracies: {val_accuracies}\", file=output_file)\n",
        "  print(f\"val_losses: {val_losses}\", file=output_file)\n",
        "  print(f\"val_true_positives: {val_true_positives}\", file=output_file)\n",
        "  print(f\"val_true_negatives: {val_true_negatives}\", file=output_file)\n",
        "  print(f\"val_false_positives: {val_false_positives}\", file=output_file)\n",
        "  print(f\"val_false_negatives: {val_false_negatives}\", file=output_file)\n",
        "\n",
        "def plot_heatmap(data_arrays, xaxes_values, yaxes_values, output_file=None):\n",
        "  fig, ax = plt.subplots()\n",
        "  im = ax.imshow(data_arrays)\n",
        "\n",
        "  plt.colorbar(im)\n",
        "  ax.set_xticks(np.arange(len(xaxes_values)))\n",
        "  ax.set_yticks(np.arange(len(yaxes_values)))\n",
        "  ax.set_xticklabels([str(x) for x in xaxes_values])\n",
        "  ax.set_yticklabels([str(y) for y in yaxes_values])\n",
        "  \n",
        "  for i in range(len(xaxes_values)):\n",
        "    for j in range(len(yaxes_values)):\n",
        "        text = ax.text(i, j, data_arrays[j, i], ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "  file_name = f\"{datetime.datetime.now()}.jpg\"\n",
        "  plt.savefig(f\"{OUTPUT_FILE_DIRECTORY}/{file_name}\")\n",
        "  print(file_name)\n",
        "  if output_file is not None:\n",
        "    print(file_name, file=output_file)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp-8t_Zdf56S"
      },
      "source": [
        "hyperparameters_sets = []\n",
        "\n",
        "for lr in lr_range:\n",
        "    for weight_decay in weight_decay_range:\n",
        "      for step_size in step_size_range:\n",
        "        set_ = {'lr': lr, 'weight_decay': weight_decay, 'step_size': step_size}\n",
        "        hyperparameters_sets.append(set_)\n",
        "        \n",
        "print(hyperparameters_sets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPmSObkPKbu3"
      },
      "source": [
        "best_accuracy = best_loss = 0.0\n",
        "best_set = {}\n",
        "best_true_positives = best_true_negatives = best_false_positives = best_false_negatives = 0\n",
        "\n",
        "val_accuracies = []; val_losses = []; val_true_positives = []; val_true_negatives = []\n",
        "val_false_positives = []; val_false_negatives = []\n",
        "\n",
        "with open(f\"{OUTPUT_FILE_DIRECTORY}/{datetime.datetime.now()}-output.txt\", \"w\") as f:\n",
        "  for set_ in hyperparameters_sets:\n",
        "    net = alexnet(pretrained=True)\n",
        "    net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    _, val_accuracy, val_loss, val_true_positive, val_true_negative, val_false_positive, val_false_negative = \\\n",
        "      train_network(net, net.parameters(), \n",
        "                    set_['lr'], NUM_EPOCHS, \n",
        "                    BATCH_SIZE, set_['weight_decay'], \n",
        "                    set_['step_size'], GAMMA, \n",
        "                    train_dataset, \n",
        "                    val_dataset=val_dataset, \n",
        "                    verbosity=True, plot=True,\n",
        "                    output_file=f)\n",
        "      \n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    val_true_positives.append(val_true_positive)\n",
        "    val_true_negatives.append(val_true_negative) \n",
        "    val_false_positives.append(val_false_positive) \n",
        "    val_false_negatives.append(val_false_negative)\n",
        "\n",
        "    if val_accuracy > best_accuracy:\n",
        "      best_accuracy = val_accuracy\n",
        "      best_loss = val_loss\n",
        "      best_true_positives = val_true_positive\n",
        "      best_true_negatives = val_true_negative\n",
        "      best_false_positives = val_false_positive\n",
        "      best_false_negatives = val_false_negative\n",
        "      best_set = copy.deepcopy(set_)\n",
        "    \n",
        "    single_iteration_log(set_, val_accuracy, val_loss, \n",
        "                         val_true_positive, val_true_negative, val_false_positive, \n",
        "                         val_false_negative, f)\n",
        "    \n",
        "    single_iteration_log(set_, val_accuracy, val_loss, \n",
        "                         val_true_positive, val_true_negative, val_false_positive, \n",
        "                         val_false_negative)\n",
        "    \n",
        "    f.flush()\n",
        "    \n",
        "  final_log(best_set, best_accuracy, best_loss, best_true_positives, \n",
        "            best_true_negatives, best_false_positives, best_false_negatives,\n",
        "            hyperparameters_sets, val_accuracies, val_losses, val_true_positives, \n",
        "            val_true_negatives, val_false_positives, val_false_negatives, f)\n",
        "  \n",
        "  final_log(best_set, best_accuracy, best_loss, best_true_positives, \n",
        "            best_true_negatives, best_false_positives, best_false_negatives,\n",
        "            hyperparameters_sets, val_accuracies, val_losses, val_true_positives, \n",
        "            val_true_negatives, val_false_positives, val_false_negatives)\n",
        "  \n",
        "  for step_size in step_size_range:\n",
        "    indexes_per_lr_value = np.empty([len(lr_range), len(weight_decay_range)], dtype=int)\n",
        "    for index, set_ in enumerate(hyperparameters_sets): \n",
        "      if set_['step_size'] == step_size:\n",
        "        lr_index = [lr_idx for lr_idx, lr in enumerate(lr_range) if lr == set_['lr']][0]\n",
        "        wd_index = [wd_idx for wd_idx, wd in enumerate(weight_decay_range) if wd == set_['weight_decay']][0]\n",
        "        indexes_per_lr_value[lr_index][wd_index] = index\n",
        "\n",
        "    np_val_accuracies = np.array([round(va, 2) for va in val_accuracies])\n",
        "    val_accuracies_heatmap = np.empty([len(lr_range), len(weight_decay_range)])\n",
        "    for idx, indexes in enumerate(indexes_per_lr_value):\n",
        "      val_accuracies_heatmap[idx] = np_val_accuracies[indexes]\n",
        "  \n",
        "  plot_heatmap(val_accuracies_heatmap, lr_range, weight_decay_range, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA5PVBitPLOf"
      },
      "source": [
        "##Specific run for testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZGyMrpJPLOq"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvEzsAcwPLOq"
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "TEST_BATCH_SIZE = 1\n",
        "EPOCHS_NUMBER = 2\n",
        "\n",
        "LR = 0.00034017498684741146\n",
        "WEIGHT_DECAY = 0.00023276171907154138\n",
        "GAMMA = 0.09288936004816421"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6krqDL7PLOq"
      },
      "source": [
        "### Dataset extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REhxjkDEPLOr"
      },
      "source": [
        "train_dataset, test_dataset = get_base_datasets()\n",
        "\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('test set {}'.format(len(test_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-pLCP63PLOr"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwxKCBBlPLOr"
      },
      "source": [
        "net = alexnet(pretrained=True)\n",
        "net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "with open(f\"{OUTPUT_FILE_DIRECTORY}/{datetime.datetime.now()}-output.txt\", \"w\") as f:\n",
        "  best_net, _, _, _, _, _, _ = train_network(net, net.parameters(), \n",
        "                                  LR, EPOCHS_NUMBER,\n",
        "                                  BATCH_SIZE, WEIGHT_DECAY, \n",
        "                                  STEP_SIZE, GAMMA,\n",
        "                                  train_dataset,\n",
        "                                  verbosity=True, plot=True,\n",
        "                                  output_file=f)\n",
        "  \n",
        "  torch.cuda.empty_cache()\n",
        "  best_net.to(DEVICE)\n",
        "  test_accuracy, test_loss, test_true_positive, test_true_negative, test_false_positive, test_false_negative = \\\n",
        "    test_network(best_net, test_dataset, TEST_BATCH_SIZE)\n",
        "\n",
        "  test_log(test_accuracy, test_loss, test_true_positive, test_true_negative,\n",
        "           test_false_positive, test_false_negative, f)\n",
        "  \n",
        "  test_log(test_accuracy, test_loss, test_true_positive, test_true_negative,\n",
        "          test_false_positive, test_false_negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN18qjPHlbfQ"
      },
      "source": [
        "## Flush mounted drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AkrLsdjla8W"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}